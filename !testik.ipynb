{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsessk/anaconda3/envs/dla/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.model.fastspeech_model import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention2(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [ (batch_size * n_heads) x seq_len x hidden_size ]\n",
    "        \n",
    "        attn = q @ k.transpose(-1, -2)\n",
    "        attn = attn / self.temperature\n",
    "        \n",
    "        # attn: [ (batch_size * n_heads) x seq_len x seq_len ]\n",
    "\n",
    "        if mask is not None:\n",
    "            attn.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = attn @ v\n",
    "\n",
    "        # output: [ (batch_size * n_heads) x seq_len x hidden_size ]\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention2(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention2(\n",
    "            temperature=d_k**0.5) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         # normal distribution initialization better than kaiming(default in pytorch)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_v))) \n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # inspired by https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        # pre-normalization for q, k, v\n",
    "        q = self.w_qs(self.layer_norm(q)).view(sz_b, len_q, n_head, d_k).transpose(1, 2)\n",
    "        k = self.w_ks(self.layer_norm(k)).view(sz_b, len_k, n_head, d_k).transpose(1, 2)\n",
    "        v = self.w_vs(self.layer_norm(v)).view(sz_b, len_v, n_head, d_v).transpose(1, 2)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, n_head, 1, 1)   # b x n x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = output + residual\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 11, 10]), torch.Size([28, 7, 11, 11]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(4 * 7, 11, 10)\n",
    "k = torch.randn(4 * 7, 11, 10)\n",
    "v = torch.randn(4 * 7, 11, 10)\n",
    "\n",
    "mla2 = MultiHeadAttention2(7, 10, 5, 5)\n",
    "output2, attn2 = mla2(q, k, v)\n",
    "output2.size(), attn2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_b, len_q, _ = q.size()\n",
    "sz_b, len_k, _ = k.size()\n",
    "sz_b, len_v, _ = v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[4], expected input with shape [*, 4], but got input of size[112, 8, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tsessk/Desktop/tts/!testik.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tsessk/Desktop/tts/%21testik.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mla \u001b[39m=\u001b[39m MultiHeadAttention(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tsessk/Desktop/tts/%21testik.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output, attn \u001b[39m=\u001b[39m mla(q, k, v)\n",
      "File \u001b[0;32m~/anaconda3/envs/dla/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/tts/src/model/fastspeech_model.py:85\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m sz_b, len_v, _ \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39msize()\n\u001b[1;32m     83\u001b[0m residual \u001b[39m=\u001b[39m q\n\u001b[0;32m---> 85\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_qs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm(q))\u001b[39m.\u001b[39mview(sz_b, len_q, n_head, d_k)\n\u001b[1;32m     86\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_ks(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(k))\u001b[39m.\u001b[39mview(sz_b, len_k, n_head, d_k)\n\u001b[1;32m     87\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_vs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(v))\u001b[39m.\u001b[39mview(sz_b, len_v, n_head, d_v)\n",
      "File \u001b[0;32m~/anaconda3/envs/dla/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dla/lib/python3.9/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/anaconda3/envs/dla/lib/python3.9/site-packages/torch/nn/functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2484\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[0;32m-> 2486\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[4], expected input with shape [*, 4], but got input of size[112, 8, 10]"
     ]
    }
   ],
   "source": [
    "mla = MultiHeadAttention(4, 4, 4, 4)\n",
    "output, attn = mla(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8, 4]), torch.Size([64, 8, 8]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size(), attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.randn(13, 20, 29, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 13, 20, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.permute(2, 0, 1, 3).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 20, 13, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.permute(2, 0, 1, 3).transpose(1, 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(754)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(f.view(29, 20, 3, 13) == f.view(29, 3, 20, 13).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(1, 10, (6, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4, 8, 1, 9, 7, 6, 1],\n",
       "        [9, 6, 3, 7, 8, 2, 8, 2],\n",
       "        [3, 7, 3, 9, 1, 3, 1, 2],\n",
       "        [9, 5, 8, 7, 5, 8, 9, 1],\n",
       "        [4, 9, 3, 6, 6, 3, 9, 4],\n",
       "        [4, 6, 8, 9, 1, 6, 4, 1]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 4, 8, 1],\n",
       "        [9, 7, 6, 1],\n",
       "        [9, 6, 3, 7],\n",
       "        [8, 2, 8, 2],\n",
       "        [3, 7, 3, 9],\n",
       "        [1, 3, 1, 2],\n",
       "        [9, 5, 8, 7],\n",
       "        [5, 8, 9, 1],\n",
       "        [4, 9, 3, 6],\n",
       "        [6, 3, 9, 4],\n",
       "        [4, 6, 8, 9],\n",
       "        [1, 6, 4, 1]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(12, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.fastspeech_model import FFTBlock\n",
    "import torch\n",
    "\n",
    "hidden_size = 16\n",
    "intermediate_size = 64\n",
    "n_head = 4\n",
    "batch_size = 4\n",
    "seq_len = 12\n",
    "\n",
    "fft_block = FFTBlock(hidden_size, intermediate_size, n_head, hidden_size // n_head, hidden_size // n_head, (1, 1), (0, 0))\n",
    "\n",
    "inp_tensor = torch.rand(batch_size, seq_len, hidden_size, dtype=torch.float32)\n",
    "\n",
    "out_tensor = fft_block(inp_tensor)[0]\n",
    "\n",
    "assert inp_tensor.shape == out_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsessk/anaconda3/envs/dla/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.model.fastspeech2 import FastSpeech2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastSpeech2(max_seq_len=3000,\n",
    "    encoder_n_layer=4,\n",
    "    vocab_size=300,\n",
    "    encoder_dim=256,\n",
    "    PAD=0,\n",
    "    encoder_conv1d_filter_size=1024,\n",
    "    encoder_head=2,\n",
    "    decoder_n_layer=4,\n",
    "    decoder_dim=256,\n",
    "    decoder_conv1d_filter_size=1024,\n",
    "    decoder_head=2,\n",
    "    fft_conv1d_kernel=[9, 1],\n",
    "    fft_conv1d_padding=[4, 0],\n",
    "    pitch_predictor_filter_size=256,\n",
    "    pitch_predictor_kernel_size=3,\n",
    "    energy_predictor_filter_size=256,\n",
    "    energy_predictor_kernel_size=3,\n",
    "    dur_predictor_filter_size=256,\n",
    "    dur_predictor_kernel_size=3,\n",
    "    num_embed=256,\n",
    "    min_pitch=59.913448819015024,\n",
    "    max_pitch=887.2688230720693,\n",
    "    min_energy=15.023643,\n",
    "    max_energy=91.4,\n",
    "    num_mels=80,\n",
    "    dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
